---
layout:     post
title:      "如何理解特征值"
subtitle:   "特征值在二次型优化和数据降维中的应用"
date:       2016-09-09
author:     "T.L"
header-img: "img/post-bg-eigenvalue.jpg"
tags:
    - 数学
---

### 定义
从线性空间的角度看，在一个定义了内积的线性空间里，对一个N阶<b>对称</b>方阵进行特征分解，就是产生了该空间的N个标准正交基，然后把矩阵投影到这N个基上。N个特征向量就是N个标准正交基，而特征值的模则代表矩阵在每个基上的投影长度。  
特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。应用到最优化中，意思就是对于R的二次型，自变量在这个方向上变化的时候，对函数值的影响最大，也就是该方向上的方向导数最大。应用到数据挖掘中，意思就是最大特征值对应的特征向量方向上包含最多的信息量，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大。  

### 应用1 二次型最优化问题
二次型$y=x^{T}Rx$，其中R是已知的二阶矩阵，R=[1，0.5；0.5，1]，$x$是二维列向量，$x=[x_1；x_2]$，求$y$的最小值。求解很简单，讲一下这个问题与特征值的关系。  
对R特征分解，特征向量是[-0.7071,0.7071]和[0.7071,0.7071]，对应的特征值分别是0.5和1.5。然后把y的等高线图画一下:  
![](http://oc5ofszxe.bkt.clouddn.com/16-9-10/57525802.jpg)  
从图中看，函数值变化最快的方向，也就是曲面最陡峭的方向，归一化以后是[0.7071；0.7071]，嗯哼，这恰好是矩阵R的一个特征值，而且它对应的特征向量是最大的。因为这个问题是二阶的，只有两个特征向量，所以另一个特征向量方向就是曲面最平滑的方向。这一点在分析最优化算法收敛性能的时候需要用到。  
二阶问题比较直观，当R阶数升高时，也是一样的道理。

### 应用2 数据降维
机器学习中的分类问题，给出178个葡萄酒样本，每个样本含有13个参数，比如酒精度、酸度、镁含量等，这些样本属于3个不同种类的葡萄酒。任务是提取3种葡萄酒的特征，以便下一次给出一个新的葡萄酒样本的时候，能根据已有数据判断出新样本是哪一种葡萄酒。  
[问题详细描述](http://archive.ics.uci.edu/ml/datasets/Wine)    
[训练样本数据](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)  
原数据有13维，但这之中含有冗余，减少数据量最直接的方法就是降维。  
做法：把数据集赋给一个178行13列的矩阵R，减掉均值并归一化(中心化)，它的协方差矩阵$C=R^{T}R$，C是13行13列的矩阵，对C进行特征分解，对角化$C=UDU^{T}$，其中U是特征向量组成的矩阵，D是特征之组成的对角矩阵，并按由大到小排列。然后，另$R' =RU$，就实现了数据集在特征向量这组正交基上的投影。嗯，重点来了，R’中的数据列是按照对应特征值的大小排列的，后面的列对应小特征值，去掉以后对整个数据集的影响比较小。比如，现在我们直接去掉后面的7列，只保留前6列，就完成了降维。这个降维方法叫PCA（Principal Component Analysis）。
以下是PCA的python代码:  

```python
from numpy import *

def pca(dataMat, topNfeat=5):
    meanVals = mean(dataMat, axis=0)
    meanRemoved = dataMat - meanVals #减去均值
    stded = meanRemoved / std(dataMat) #用标准差归一化
    covMat = cov(stded, rowvar=0) #求协方差方阵
    eigVals, eigVects = linalg.eig(mat(covMat)) #求特征值和特征向量
    eigValInd = argsort(eigVals)  #对特征值进行排序
    eigValInd = eigValInd[:-(topNfeat + 1):-1]  
    redEigVects = eigVects[:, eigValInd]       # 除去不需要的特征向量
    lowDDataMat = stded * redEigVects    #求新的数据矩阵
    reconMat = (lowDDataMat * redEigVects.T) * std(dataMat) + meanVals
    return lowDDataMat, reconMat
```   
下面看结果：  
![](http://oc5ofszxe.bkt.clouddn.com/16-9-10/84800048.jpg)  
这是不降维时候的分类错误。  
![](http://oc5ofszxe.bkt.clouddn.com/16-9-10/5993012.jpg)
这是降维以后的分类错误率。

结论：降维以后分类错误率与不降维的方法相差无几，但需要处理的数据量减小了一半（不降维需要处理13维，降维后只需要处理6维。
