<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="iCQLNHIUn9" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="云计算大数据博客|陆志君,web,数据挖掘与数据分析,分布式算法|这里是 @T.L陆志君的个人博客，与你一起发现更大的世界">
    <meta name="keyword"  content="T.L,tl,luzhijun,trucy,trucyluce,陆志君,陆志君的博客,大数据，云计算,分布式算法,博客,花梦的男朋友">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>微博话题爬取与存储分析 - 楚汐|Trucy Luce Blog</title>

    <link rel="canonical" href="http://localhost:4000/2016/10/29/%E5%BE%AE%E5%8D%9A%E8%AF%9D%E9%A2%98%E7%88%AC%E5%8F%96%E4%B8%8E%E5%AD%98%E5%82%A8%E5%88%86%E6%9E%90/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script>
    var _hmt = _hmt || [];
    (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?c07b62da6a94126e41877e6b30e15413";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
    })();
    </script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">T.L Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-spider.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        background-image: url('/img/post-bg-spider.jpg')
    }
</style>
<header class="intro-header" >
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#社交网站数据分析" title="社交网站数据分析">社交网站数据分析</a>
                        
                        <a class="tag" href="/tags/#社会计算" title="社会计算">社会计算</a>
                        
                        <a class="tag" href="/tags/#爬虫" title="爬虫">爬虫</a>
                        
                        <a class="tag" href="/tags/#大数据" title="大数据">大数据</a>
                        
                    </div>
                    <h1>微博话题爬取与存储分析</h1>
                    
                    
                    <h2 class="subheading">一步步教你微博话题数据爬取与分析，以上海租房为例</h2>
                    
                    <span class="meta">Posted by T.L on October 29, 2016</span>
                </div>
            </div>
        </div>
    </div>
<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {
  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
  } 
}); 
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
<!-- 百度自动推送 -->
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

				<p>大数据社会下数据就是黄金，新浪微博作为一个国内网络社交早就意识到这一点，本着资本家和商人的心态给你提供的开放API接口只可以获得少量无关紧要的数据（想要数据，money来换），对比国外Twitte等社交平台会提供一些数据接口供研究人员获取大量研究数据。那我们GEEK的口号是，凡是网上能显示数据的朕兼“可取”(v_v…为什么加个引号呢，因为虽然出于技术角度是都可取得，但出于道德方面考虑也要尊重数据作者的规约）。</p>

<p>本文基于python以新浪微博为数据平台，从数据采集、关键字提取、数据存储三个角度，用最简单的策略来挖掘我们的“黄金”。</p>

<p>有爬虫基础的人可以直接跳过数据采集部分看“上海租房”话题挖掘实战项目，项目地址<a href="https://github.com/luzhijun/weiboSA">https://github.com/luzhijun/weiboSA</a>（目前已更新豆瓣小组爬取）。</p>

<h2 id="数据采集">数据采集</h2>
<p>使用python是因为代码简洁，虽然计算比java和c慢很多，但数据采集时间开销大部分是IO部分的，你愿意每次用java或者c写效率也提高不到哪去。</p>

<p>数据采集基本用爬虫机器人，原理谁都会，google就是靠他发家致富走上人生巅峰的。下面介绍常用来做爬虫的几个库。</p>

<h3 id="urllib">Urllib</h3>

<p>怎样抓网页呢？其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我们就写个例子来扒一个网页下来。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">urllib2</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">urllib2</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="s">"http://www.baidu.com"</span><span class="p">)</span>
<span class="k">print</span> <span class="n">response</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></td></tr></tbody></table>
</div>
</div>
<p>结果就和在Chrome等浏览器中右键查看源码一样的内容，urllib2是python内置库，简化了httplib的用法(urllib2.urlopen相当于Java中的HttpURLConnection)。有2那肯定有urllib啊，urllib2可以接受一个Request类的实例来设置URL请求的headers，但urllib仅可以接受URL。这意味着，你不可以伪装你的User Agent字符串等。urllib2在python3.x中被改为urllib.request。
接下来用urllib2伪装iphone 6浏览，模拟浏览器发送GET请求。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5</pre></td><td class="code"><pre><span class="n">req</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">'http://www.douban.com/'</span><span class="p">)</span>
<span class="n">req</span><span class="o">.</span><span class="n">add_header</span><span class="p">(</span><span class="s">'User-Agent'</span><span class="p">,</span> <span class="s">'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'</span><span class="p">)</span>
<span class="k">with</span> <span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">req</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Status:'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">status</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">reason</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Data:'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
</pre></td></tr></tbody></table>
</div>
</div>
<p>结果会返回移动版的源码信息</p>

<div class="highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4</pre></td><td class="code"><pre>...
&lt;link rel="apple-touch-icon-precomposed" href="https://gss0.bdstatic.com/5bd1bjqh_Q23odCf/static/wiseindex/img/screen_icon.png"/&gt;  
&lt;meta name="format-detection" content="telephone=no"/&gt;
...
</pre></td></tr></tbody></table>
</div>
</div>

<p>如果想要以post方式提交，只要在Request中附加data字段就可以，下面附加用户名密码登录新浪博客。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25</pre></td><td class="code"><pre><span class="c">#我们模拟一个微博登录，先读取登录的邮箱和口令，然后按照weibo.cn的登录页的格式以username=xxx&amp;password=xxx的编码传入：</span>
<span class="kn">from</span> <span class="nn">urllib</span> <span class="kn">import</span> <span class="n">parse</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Login to weibo.cn...'</span><span class="p">)</span>
<span class="n">email</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Email: '</span><span class="p">)</span>
<span class="n">passwd</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Password: '</span><span class="p">)</span>
<span class="n">login_data</span> <span class="o">=</span> <span class="n">parse</span><span class="o">.</span><span class="n">urlencode</span><span class="p">([</span>
    <span class="p">(</span><span class="s">'username'</span><span class="p">,</span> <span class="n">email</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'password'</span><span class="p">,</span> <span class="n">passwd</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'entry'</span><span class="p">,</span> <span class="s">'weibo'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'client_id'</span><span class="p">,</span> <span class="s">''</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'savestate'</span><span class="p">,</span> <span class="s">'1'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'ec'</span><span class="p">,</span> <span class="s">''</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'pagerefer'</span><span class="p">,</span> <span class="s">'https://passport.weibo.cn/signin/welcome?entry=mweibo&amp;r=http</span><span class="si">%3</span><span class="s">A</span><span class="si">%2</span><span class="s">F</span><span class="si">%2</span><span class="s">Fm.weibo.cn</span><span class="si">%2</span><span class="s">F'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">req</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s">'https://passport.weibo.cn/sso/login'</span><span class="p">)</span>
<span class="n">req</span><span class="o">.</span><span class="n">add_header</span><span class="p">(</span><span class="s">'Origin'</span><span class="p">,</span> <span class="s">'https://passport.weibo.cn'</span><span class="p">)</span>
<span class="n">req</span><span class="o">.</span><span class="n">add_header</span><span class="p">(</span><span class="s">'User-Agent'</span><span class="p">,</span> <span class="s">'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'</span><span class="p">)</span>
<span class="n">req</span><span class="o">.</span><span class="n">add_header</span><span class="p">(</span><span class="s">'Referer'</span><span class="p">,</span> <span class="s">'https://passport.weibo.cn/signin/login?entry=mweibo&amp;res=wel&amp;wm=3349&amp;r=http</span><span class="si">%3</span><span class="s">A</span><span class="si">%2</span><span class="s">F</span><span class="si">%2</span><span class="s">Fm.weibo.cn</span><span class="si">%2</span><span class="s">F'</span><span class="p">)</span>

<span class="k">with</span> <span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">login_data</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Status:'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">status</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">reason</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">getheaders</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s: </span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Data:'</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
</pre></td></tr></tbody></table>
</div>
</div>
<p>其中Origin和referer字段是反“反盗链”，就是检查你发送请求的header里面，referer站点是不是他自己。</p>

<h3 id="cookielib">Cookielib</h3>

<p>爬虫被封的一个依据就是重复IP，因此可以为爬虫设置不同代理IP。此外有些网站需要cookie才能查看，所谓Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib2库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。</p>

<p>cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源。Cookielib模块非常强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。</p>

<p>它们的关系：CookieJar–派生-&gt;FileCookieJar –派生–&gt;MozillaCookieJar和LWPCookieJar</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9</pre></td><td class="code"><pre><span class="kn">from</span> <span class="nn">urllib</span> <span class="kn">import</span> <span class="n">request</span>
<span class="kn">from</span> <span class="nn">http.cookiejar</span> <span class="kn">import</span> <span class="n">CookieJar</span>

<span class="n">cookie</span><span class="o">=</span><span class="n">CookieJar</span><span class="p">()</span>
<span class="n">cookie_support</span><span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">HTTPCookieProcessor</span><span class="p">(</span><span class="n">cookie</span><span class="p">)</span><span class="c">#cookie处理器</span>
<span class="n">opener</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">build_opener</span><span class="p">(</span><span class="n">cookie_support</span><span class="p">)</span>
<span class="n">opener</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'http://www.baidu.com'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">cookie</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="s">':'</span><span class="p">,</span><span class="n">item</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></td></tr></tbody></table>
</div>
</div>

<p>结果：</p>
<blockquote>
  <p>BAIDUID : E4DECD4AF63915B9AFF5AC28951A3DAA:FG=1<br />
BIDUPSID : E4DECD4AF63915B9AFF5AC28951A3DAA<br />
H_PS_PSSID : 1437_18241_17944_21079_18559_21454_21406_21377_21191_21321<br />
PSTM : 1477631558<br />
BDSVRTM : 0<br />
BD_HOME : 0</p>
</blockquote>

<p>这里使用默认的CookieJar 对象，如果要将Cookie保存起来，可以使用FileCookieJar类和其子类中的save方法，加载就用load方法。</p>

<p>写脚本从指定网站抓取数据的时候，免不了会被网站屏蔽IP。所以呢，就需要有一些IP代理。随便在网上找了一个提供免费IP的网站<a href="http://www.xicidaili.com/">西刺</a>做IP抓取。观察可以发现有我们需要的信息的页面url有下面的规律：www.xicidaili.com/nn/+页码。可是你如果直接通过get方法访问的话你会发现会出现500错误。原因其实出在这个规律下的url虽然都是get方法获得数据，但都有cookie认证，另外还有反外链等，下面例子用来获得西刺的cookie。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10</pre></td><td class="code"><pre><span class="n">headers</span><span class="o">=</span><span class="p">[(</span><span class="s">'User-Agent'</span><span class="p">,</span><span class="s">'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'Host'</span><span class="p">,</span><span class="s">'www.xicidaili.com'</span><span class="p">),</span>
    <span class="p">(</span><span class="s">'Referer'</span><span class="p">,</span><span class="s">'http://www.xicidaili.com/n'</span><span class="p">)]</span>
<span class="k">def</span> <span class="nf">getCookie</span><span class="p">()</span>
    <span class="n">cookie</span><span class="o">=</span><span class="n">CookieJar</span><span class="p">()</span>
    <span class="n">cookie_support</span><span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">HTTPCookieProcessor</span><span class="p">(</span><span class="n">cookie</span><span class="p">)</span><span class="c">#cookie处理器</span>
    <span class="n">opener</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">build_opener</span><span class="p">(</span><span class="n">cookie_support</span><span class="p">)</span>
    <span class="n">opener</span><span class="o">.</span><span class="n">addheaders</span><span class="o">=</span><span class="n">headers</span>
    <span class="n">opener</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'http://www.xicidaili.com/'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cookie</span>
</pre></td></tr></tbody></table>
</div>
</div>

<p>有了cookie就可以爬了，爬的内容怎么处理呢，介绍个SB工具—— BeautifulSoup。</p>

<h3 id="beautifulsoup">BeautifulSoup</h3>
<p><a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>翻译叫鸡汤，现在版本是4.5.1，简称BS4，倒过来叫4SB，不过抓数据一点都不SB。提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。
关于BS的介绍和用法<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">官方文档</a>很详细，下面给几个”Web scraping with python”<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>中的例子看下BS是否好喝，可以和文档对照看。
首先你得安装了BS，然后爬取<a href="http://www.pythonscraping.com/pages/page3.html">http://www.pythonscraping.com/pages/page3.html</a>中的图片来小试牛刀。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">urllib</span> <span class="kn">import</span> <span class="n">request</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="n">html</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="s">"http://www.pythonscraping.com/pages/page3.html"</span><span class="p">)</span>
<span class="n">bs</span><span class="o">=</span><span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html</span><span class="p">,</span><span class="s">"lxml"</span><span class="p">)</span>
<span class="c">#打印所有图片地址</span>
<span class="k">for</span> <span class="n">pic</span> <span class="ow">in</span> <span class="n">bs</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s">'img'</span><span class="p">,{</span><span class="s">'src'</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">".*</span><span class="err">\</span><span class="s">.jpg$"</span><span class="p">)}):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pic</span><span class="p">[</span><span class="s">'src'</span><span class="p">])</span>
</pre></td></tr></tbody></table>
</div>
</div>

<p>结果:</p>
<blockquote>
  <p>../img/gifts/logo.jpg<br />
../img/gifts/img1.jpg<br />
../img/gifts/img2.jpg<br />
../img/gifts/img3.jpg<br />
../img/gifts/img4.jpg<br />
../img/gifts/img6.jpg</p>
</blockquote>

<p>接上文，我们把西刺的高匿代理ip爬出来放到本地proxy.txt。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></td><td class="code"><pre><span class="n">cookie</span><span class="o">=</span><span class="n">getCookie</span><span class="p">()</span>
<span class="c"># get the proxy</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'proxy.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">101</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">page</span><span class="o">%</span><span class="mi">50</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span><span class="c">#每50页更新下cookie</span>
            <span class="n">cookie</span><span class="o">=</span><span class="n">getCookie</span><span class="p">()</span>

        <span class="n">url</span> <span class="o">=</span> <span class="s">'http://www.xicidaili.com/nn/</span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span><span class="n">page</span>
        <span class="n">cookie_support</span><span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">HTTPCookieProcessor</span><span class="p">(</span><span class="n">cookie</span><span class="p">)</span>
        <span class="n">opener</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">build_opener</span><span class="p">(</span><span class="n">cookie_support</span><span class="p">)</span>
        <span class="n">request</span><span class="o">.</span><span class="n">install_opener</span><span class="p">(</span><span class="n">opener</span><span class="p">)</span>

        <span class="n">req</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span><span class="n">headers</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">headers</span><span class="p">))</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">content</span><span class="p">,</span><span class="s">"lxml"</span><span class="p">)</span>
        <span class="n">trs</span> <span class="o">=</span> <span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s">'table'</span><span class="p">,</span><span class="nb">id</span><span class="o">=</span><span class="s">"ip_list"</span><span class="p">)</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">'tr'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tr</span> <span class="ow">in</span> <span class="n">trs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">tds</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">findAll</span><span class="p">(</span><span class="s">'td'</span><span class="p">)</span>
            <span class="n">ip</span> <span class="o">=</span> <span class="n">tds</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">port</span> <span class="o">=</span> <span class="n">tds</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">protocol</span> <span class="o">=</span> <span class="n">tds</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">s://</span><span class="si">%</span><span class="s">s:</span><span class="si">%</span><span class="s">s</span><span class="se">\n</span><span class="s">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">protocol</span><span class="p">,</span> <span class="n">ip</span><span class="p">,</span> <span class="n">port</span><span class="p">))</span>
</pre></td></tr></tbody></table>
</div>
</div>
<p>结果十五秒爬了1万条数据（与电脑环境有关），说明1页正好100条，而总页数超过1000页，也就是记录数超过10w条，如果固定用同一个cookie肯定不安全（谁会有空翻看1000页数据。。。），因此设置每爬50页更新下cookie。
有了代理地址，不一定能保证有效，可能就被封杀了，因此使用思路是把代理地址存入哈希表，验证无效的删除（看状态码），重新在表中取新的记录。
代理地址使用方式如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4</pre></td><td class="code"><pre><span class="o">...</span>
<span class="n">proxy_handler</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">ProxyHandler</span><span class="p">({</span><span class="s">'http'</span><span class="p">:</span> <span class="s">'123.165.121.126:81'</span><span class="p">})</span> <span class="c">#http://www.xicidaili.com/nn/2 随便找个</span>
<span class="n">opener</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">build_opener</span><span class="p">(</span><span class="n">proxy_handler</span><span class="p">,</span><span class="n">cookie_handler</span> <span class="o">...</span><span class="err">各种其他</span><span class="n">handle</span><span class="p">)</span>
<span class="o">...</span>
</pre></td></tr></tbody></table>
</div>
</div>
<p>另外推荐个神器，<a href="https://my.oschina.net/jhao104/blog/512384">crawlera</a> ，基本满足各种需要。</p>

<p>假如真要爬1000页，需要花150秒？好吧，好像也不多，但我要说的是可以多进程或者异步处理。多进程很好做，注意以手动维护一个HttpConnection的池，然后每次抓取时从连接池里面选连接进行连接即可（每秒几百个连接正常的有理智的服务器一定会封禁你的）。python的异步处理用到了Twisted库，却远没有同是异步模式的nodejs火，算是python中的巨型框架了，想想python的巨型框架活的不久，感兴趣的推荐看下《Twisted网络编程必备》<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>。关于单线程、多线程、异步有张图推荐看下。</p>

<p><img src="http://oc5ofszxe.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-28%20%E4%B8%8B%E5%8D%883.37.06.png" alt="" /></p>

<p>写爬虫还要考虑其他很多问题，授权验证、连接池、数据处理、js处理等，这里有个经典爬虫框架：Scrapy，目前支持python3，支持分布式， 使用 Twisted来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。</p>

<h3 id="scrapy与pyspider">Scrapy与Pyspider</h3>
<p>Scrapy的入门学习参见<a href="http://www.jianshu.com/p/a8aad3bf4dc4">学习Scrapy入门</a>，对应<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html">中文文档</a>几小时内可以快速掌握。另外国内某大神开发了个WebUI的<a href="https://github.com/binux/pyspider">Pyspider</a>，具有以下特性：</p>

<ol>
  <li>python 脚本控制，可以用任何你喜欢的html解析包（内置 pyquery）</li>
  <li>WEB 界面编写调试脚本，起停脚本，监控执行状态，查看活动历史，获取结果产出</li>
  <li>支持 MySQL, MongoDB, SQLite</li>
  <li>支持抓取 JavaScript 的页面</li>
  <li>组件可替换，支持单机/分布式部署，支持 Docker 部署</li>
  <li>强大的调度控制</li>
</ol>

<p>从内容上讲，两者具有功能差不多，包括以上3，5，6。不同是Scrapy原生不支持js渲染，需要单独下载<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>,而PyScrapy内置支持<a href="https://github.com/scrapinghub/scrapyjs">scrapyjs</a>；PySpider内置 pyquery选择器，Scrapy有XPath和CSS选择器，这两个大家可能更熟一点；此外，Scrapy全部命令行操作，Pyscrapy有较好的WebUI；还有，scrapy对千万级URL去重支持很好，采用<a href="https://luzhijun.github.io/2016/09/02/海量数据处理/#bloom-filter">布隆过滤</a>来做，而Spider用的是数据库来去重？最后，PySpider更加容易调试，scrapy默认的debug模式信息量太大，warn模式信息量太少，由于异步框架出错后是不会停掉其他任务的，也就是出错了还会接着跑。。。从整体上来说，pyspider比scrapy简单，并且pyspider可以在线提供爬虫服务，也就是所说的SaaS，想要做个简单的爬虫推荐使用它，但自定义程度相对scrapy低，社区人数和文档都没有scrapy强，但scrapy要学习的相关知识也较多，故而完成一个爬虫的时间较长。</p>

<p>因为比较喜欢有完整文档的支持，所以后面主要用Scrapy，简要说下Scrapy运行流程。</p>

<ul>
  <li>首先，引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
  <li>引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)</li>
  <li>然后，爬虫解析Response</li>
  <li>若是解析出实体（Item）,则交给实体管道进行进一步的处理。</li>
  <li>若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取</li>
</ul>

<p>根据scrapy文档<a href="http://doc.scrapy.org/en/master/topics/practices.html#avoiding-getting-banned">描述</a>,要防止scrapy被禁用，主要有以下几个策略。</p>

<ol>
  <li>动态设置user agent</li>
  <li>禁用cookies</li>
  <li>设置延迟下载</li>
  <li>使用 <a href="http://www.googleguide.com/cached_pages.html">Google cache</a></li>
  <li>使用IP地址池（ <a href="https://www.torproject.org/">Tor project</a> 、VPN和代理IP）</li>
  <li>使用 <a href="http://scrapinghub.com/crawlera">Crawlera</a></li>
</ol>

<p>由于Google cache基于你懂的原因不可用，其余都可以利用，Crawlera的分布式下载，我们可以在下次用一篇专门的文章进行讲解。下面主要从动态随机设置user agent、禁用cookies、设置延迟下载和使用代理IP这几个方式入手。</p>

<p><strong>自定义中间件</strong></p>

<p>Scrapy下载器通过中间件控制的，要实现代理IP、user agent切换可以自定义个中间件。
在项目下创建（如何创建项目，使用scrapy start yourProject命令，参考文档）好项目后，在里面找到setting.py文件，先把agents和代理ip放到setting.py中（代理ip较少情况下这样做，较多的话还是放到数据库中去，方便管理）,设置中间件名字MyCustomSpiderMiddleware和优先级。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37</pre></td><td class="code"><pre><span class="n">USER_AGENTS</span> <span class="o">=</span> <span class="p">[</span>
	<span class="s">"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span><span class="p">,</span>
	<span class="s">"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)"</span><span class="p">,</span>
	<span class="s">"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)"</span><span class="p">,</span>
	<span class="s">"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11"</span><span class="p">,</span>
	<span class="s">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20"</span><span class="p">,</span>
	<span class="s">"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">PROXIES</span> <span class="o">=</span> <span class="p">[</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'111.11.228.75:80'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'120.198.243.22:80'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'111.8.60.9:8123'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'101.71.27.120:80'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'122.96.59.104:80'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
	<span class="p">{</span><span class="s">'ip_port'</span><span class="p">:</span> <span class="s">'122.224.249.122:8088'</span><span class="p">,</span> <span class="s">'user_pass'</span><span class="p">:</span> <span class="s">''</span><span class="p">},</span>
<span class="p">]</span>
<span class="c"># 禁用cookoe (enabled by default)</span>
<span class="n">COOKIES_ENABLED</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c">#设置下载延迟</span>
<span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c"># 下载中间件</span>
<span class="c"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span>
<span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'weiboZ.middlewares.MyCustomDownloaderMiddleware'</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></td></tr></tbody></table>
</div>
</div>

<p>middlewares/MyCustomDownloaderMiddleware.py</p>

<div class="language-python highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22</pre></td><td class="code"><pre><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">from</span> <span class="nn">settings</span> <span class="kn">import</span> <span class="n">PROXIES</span>
<span class="k">class</span> <span class="nc">RandomUserAgent</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="s">"""Randomly rotate user agents based on a list of predefined ones"""</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agents</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">agents</span> <span class="o">=</span> <span class="n">agents</span>
	<span class="nd">@classmethod</span>
	<span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">getlist</span><span class="p">(</span><span class="s">'USER_AGENTS'</span><span class="p">))</span>
	<span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
		<span class="c">#随机选个agent</span>
		<span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s">'User-Agent'</span><span class="p">,</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agents</span><span class="p">))</span>
<span class="k">class</span> <span class="nc">ProxyMiddleware</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
		<span class="n">proxy</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">PROXIES</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">proxy</span><span class="p">[</span><span class="s">'user_pass'</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
			<span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">'proxy'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"http://</span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="n">proxy</span><span class="p">[</span><span class="s">'ip_port'</span><span class="p">]</span>
			<span class="n">encoded_user_pass</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">encodestring</span><span class="p">(</span><span class="n">proxy</span><span class="p">[</span><span class="s">'user_pass'</span><span class="p">])</span>
			<span class="n">request</span><span class="o">.</span><span class="n">headers</span><span class="p">[</span><span class="s">'Proxy-Authorization'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'Basic '</span> <span class="o">+</span> <span class="n">encoded_user_pass</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">request</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s">'proxy'</span><span class="p">]</span> <span class="o">=</span> <span class="s">"http://</span><span class="si">%</span><span class="s">s"</span> <span class="o">%</span> <span class="n">proxy</span><span class="p">[</span><span class="s">'ip_port'</span><span class="p">]</span>
</pre></td></tr></tbody></table>
</div>
</div>

<h3 id="互联网道德和规约">互联网道德和规约</h3>

<p>当你准备爬某个网站的时候，首先应该先看下该网站有没有robots.txt。robots.txt是1994年出现的，也称为机器人排除标准(Robots Exclusion Standard)，网站管理员不想某些内容被爬到的时候可以再该文件中注明。robots.txt虽然有主流的语法格式，但是各大企业标准不一，没有别人可以阻止你创建自己版本的robots.txt，但这些robots.txt不应该因为不符合主流而不被遵守。一般文件字段包含：User-agent，Allow，Disallow分别代表搜索机器人允许看和不许看的内容。</p>

<p>之前看新闻说今年4月大众点评把百度给告了，请求法院判令两被告停止不正当竞争行为，共同赔偿汉涛公司经济损失9000万元和为制止侵权行为支出的45万余元，并刊登公告、澄清事实消除不良影响。有用百度地图的应该知道这个（最近百度高德开撕，又在黑百度了~~~），定位完毕会显示附近商家和点评信息，来看下大众点评网的<a href="http://www.dianping.com/robots.txt">robots.txt</a>.
光看</p>

<blockquote>
  <p>User-agent: *<br />
…<br />
Disallow: /shop/<em>/rank_p</em><br />
…</p>
</blockquote>

<p>就知道不允许任何企业和个人爬他家的商店评分数据，更何况其他更具有价值的数据呢，数据是黄金，要求赔偿9000万元对百度来说不算多，但百度回应内容大众点评网的robots协议面向百度等搜索引擎开放，百度地图抓取大众点评网的内容正是在robots.txt允许的情况下。通常业内习惯上没有被不允许的就是允许的，也就是说网站的关键信息可以帮助SEO优化的这个不能被禁止哟，不然你就没头条了，看人家竞争对手爱帮网倒是单独被列出来全面封杀了，因为其实力太弱，没有商业合作价值。就算这样我也没看出允许百度抓点评的用户评论数据，难道说点评网之前没robots.txt？人家不傻！百度挖了人家数据还叫嚣着遵守Robots协议，（其实他完全可以偷偷摸摸抓了数据自己私下研究，却要直接在百度地图上显示出来，这是要把数据价值榨干啊，够霸道）好比把人打了顿理直气壮地说你瞅啥一样，太野蛮了。。。</p>

<p>说多了，来看下新浪微博的Robots<a href="http://weibo.com/robots.txt">协议</a>。明确规定了Sitemap: http://weibo.com/sitemap.xml 中列出的内容不允许被百度、360、谷歌、搜狗、微软必应、好搜、神马查看，后面还注明了Disallow: User-agent: * Disallow: /，也就是说前面是单独列出的，理论上这些数据不允许任何机构和个人爬取。这些是啥数据呢，movie和music数据，那你放心好了，微博文本数据可以爬了，但人家也不傻，可以显示的微博信息是有限制的，不可能所有数据库的数据都显示出来。</p>

<h3 id="实战">实战</h3>
<p>在58、赶集、链家上找过房子的人都为中介苦恼，所谓的行业规矩令人做呕，这些人不生产社会价值却担当了新世纪的买办角色，好在通过微博也可以找房，而且绝大部分是个人房源。</p>

<p>以上海找房子为例，微博搜索框输入@上海租房 就可以的到如下页面</p>
<blockquote>
  <p><a href="http://s.weibo.com/weibo/%2540%25E4%25B8%258A%25E6%25B5%25B7%25E7%25A7%259F%25E6%2588%25BF?topnav=1&amp;wvr=6&amp;b=1">http://s.weibo.com/weibo/%2540%25E4%25B8%258A%25E6%25B5%25B7%25E7%25A7%259F%25E6%2588%25BF?topnav=1&amp;wvr=6&amp;b=1</a></p>
</blockquote>

<p>还是不错的，然后看下源码发现并没有html数据，显然是AJAX异步了，Scrapy要爬的话还得安装scrapy-splash改下配置用splash解析js内容，而且要看下一页必须登录状态才可以，那要在header里面添加cookie，可以登录后chrome F12 开发工具查看，但你敢保证拿包含自己的账号的cookie去做爬虫发现了不被封？其实这里可以显示的数据最多1000条，按最新的1000条显示，何必大费周章去搞那么复杂呢，可以用移动版的微博搜下嘛,<a href="http://m.weibo.cn/main/pages/index?  containerid=100103type%3D1%26q%3D%40%E4%B8%8A%E6%B5%B7%E7%A7%9F%E6%88%BF&amp;type=all&amp;queryVal=%40%E4%B8%8A%E6%B5%B7%E7%A7%9F%E6%88%BF&amp;featurecode=20000180&amp;oid=4035270175262125&amp;luicode=20000061&amp;lfid=4035270175262125&amp;title=%40%E4%B8%8A%E6%B5%B7%E7%A7%9F%E6%88%BF">点击</a>。</p>

<p>用开发者工具看下网络请求数据状况，搜索包含名字‘page’ 请求消息头，可以发现规律：</p>

<p><img src="http://oc5ofszxe.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-28%20%E4%B8%8B%E5%8D%889.10.36.png" alt="" /></p>

<p>左边Name列凡是内容页下拉引起ajax加载新页，新页内容以json格式返回；右边字段末尾page=？部分，代表传递第几页的内容，?最大到100，和电脑版最多看50页一样有数据限制。<br />
json内容如下：</p>

<p><img src="http://oc5ofszxe.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202016-10-28%20%E4%B8%8B%E5%8D%889.16.19.png" alt="" /></p>

<p>ok，能显示的数据都在里面，而且还是json格式，都不用选择器了，这个要比电脑版简单多了。</p>

<h2 id="数据提取etl">数据提取（ETL）</h2>

<h3 id="选择需要的数据">选择需要的数据</h3>

<p>并不是所有json字段的数据都有用，这里只选取有用的字段，总的原则是按需抽取。可以看下项目中定义的<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/items.py">Items.py</a>。</p>

<p>微博内容id 对应字段放数据库中将有唯一约束，防止重复微博。选择mblogid作为唯一id，而千万不是itemid，经测试发现itemid只代表当天微博的槽位，比如限制浏览10条数据，就有1~10个槽位，而itemid就代表这10个槽位标签，并不代表微博内容id。另外mblog字段下还有个id属性，估计和mblogid一样的效果，有兴趣可以试试。<br />
发布时间代表信息的实效，json里面有两个字段表示，一个是时间戳created_timestamp，另一个是显示出来的真实时间数据，这里取真实数据方便直接提取显示，但后期存储的时候需要统一转换为标准时间格式。 <br />
评论数、转发数、点赞数和时效结合可以用来综合评估微博信息价值(时间越靠后这三个数字越能评价信息价值)。<br />
  用户名、粉丝数、说说数可以用来检验用户是否有价值用户，或者是机器人。<br />
后期处理需要提取求/租信息的关键词，包含价格、几号线、行政区划、信息是求租还是出租。</p>

<p>项目中定义的<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/pipelines.py">pipelines.py</a>文件是scrapy管道处理类，也就是主要的后期数据处理类。其中一个是JsonPipeline类，直接将数据打印到json文件中，这个前期可以用来调试爬虫效果。另一个是MongoPipeline类，用来保存后期处理后的数据。在setting文件中ITEM_PIPELINES属性可以设置具体采用哪个管道处理类。</p>

<p>后期处理主要任务是提取关键字，如何从微博信息中爬取地理位置、价格？这里采用双数组Trie树</p>

<h3 id="dat">DAT</h3>

<p>Trie树是搜索树的一种，来自英文单词”Retrieval”的简写，可以建立有效的数据检索组织结构，是中文匹配分词算法中词典的一种常见实现。它本质上是一个确定的有限状态自动机（DFA），每个节点代表自动机的一个状态。在词典中这此状态包括“词前缀”，“已成词”等。前面文章讲了下其原理，可以<a href="https://luzhijun.github.io/2016/09/02/海量数据处理/#trie">查看</a>。</p>

<p>采用Trie树搜索最多经过n次匹配即可完成一次查找(即最坏是0(n))，而与词库中词条的数目无关，缺点是空间空闲率高，它是中文匹配分词算法中词典的一种常见实现。</p>

<p>双数组Trie（doublearrayTrie,DAT）是trie树的一个简单而有效的实现（日本人发明的），由两个整数数组构成，一个是base[]，另一个是check[]。双数组Trie树是Trie树的一种变形，是在保证Trie树检索速度的前提下，提高空间利用率而提出的一种数据结构.其本质是一个确定有限状态自动机(DeterministicFiniteAutomaton，DFA)，每个节点代表自动机的一个状态，根据变量的不同，进行状态转移，当到达结束状态或者无法转移时完成查询.DAT采用两个线性数组(base和check)对Trie树保存，base和check数组拥有一致的下标，即DFA中的每一个状态，也即Trie树中所说的节点，base数组用于确定状态的转移，check数组用于检验转移的正确性，检验该状态是否存在<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup>。</p>

<p>在比较用于正向最大匹配分词的速度方面，DAT分词平均速度为936kB/s<sup id="fnref:4"><a href="#fn:4" class="footnote">5</a></sup>（2006年），项目用到github上一日本人的python版的<a href="https://github.com/pytries/datrie">DAT</a>，其查询速度可以达到 2.755M/s，查询速度和分词速度基本是差不多的，这三倍的差距应该是做了优化的。</p>

<p>词典的收集是比较麻烦，没有现成的，项目中搜集了上海地铁、街道、行政区、乡镇等信息，其中价格信息范围是从600~9000，可识别二千、二千二、两千一等中文价格，后面微博上看到有人用1.2k做价格的，暂时没加入，自己可以加入词条后重新运行下makeData.py文件即可收录。</p>

<p>判断信息是租房还是求房也是根据关键字，当信息中出现[“求租”, “想租”,”求到”,”求从”, “要租”, “寻租”,”寻找”, “找新房子”, “找房子”, “找房”, “寻房”, “求房”, “想找”, “希望房”]信息就标注为求房，否则标注为租房。</p>

<p>此外项目还收集了三千多个<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/上海楼盘.py">楼盘信息</a>，由于有些楼盘信息容易混淆真实语境，比如‘峰会’（真不懂怎么会有这楼盘名）、‘艺品’与信息‘文艺品味’、‘黄兴’、‘金铭’与人名冲突等等。有想根据楼盘查询信息的同学可以把<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/makeData.py">makeData.py</a>中第5、51行注释取消运行下这个文件。</p>

<p>关于时间处理，微博挖到的时间有几种类型：</p>

<ul>
  <li>2016年01月01日 00点00分</li>
  <li>1月1日 00：00</li>
  <li>今天 00：00</li>
  <li>1分钟前/11分钟前</li>
  <li>10秒前</li>
</ul>

<p>需要统一转化，使用DataUtil类处理。其中mongodb使用的是ISO时间，比北京时间早8小时，而pymongo中的datetime.datetime 数据并不会按时区处理，因此手动减少8小时后存储。同样从mongoDB中取出的时间要转化为当地时间。</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5</pre></td><td class="code"><pre><span class="o">&gt;</span> <span class="nx">d</span><span class="o">=</span><span class="k">new</span> <span class="nb">Date</span><span class="p">()</span>
<span class="o">&gt;</span> <span class="nx">d</span>
<span class="nx">ISODate</span><span class="p">(</span><span class="s2">"2016-10-29T06:59:49.461Z"</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="nx">d</span><span class="p">.</span><span class="nx">toLocaleDateString</span><span class="p">()</span>
<span class="mi">10</span><span class="o">/</span><span class="mi">29</span><span class="o">/</span><span class="mi">2016</span>
</pre></td></tr></tbody></table>
</div>
</div>

<h2 id="数据存储">数据存储</h2>

<p>其实就这点数据放哪个数据库都无所谓，但假如这个数据量很大，就要好好考虑数据存储了。</p>

<h3 id="选择oraclemysql-还是-nosql">选择oracle、mysql 还是 nosql</h3>

<p>数据库的比较就好比java、c#、python、Go等的骂战一样，没有最好的，只有最适合场景的。oracle、mysql都学过，nosql中学过hbase和mongodb，就我而言单从7个角度比较：</p>

<ol>
  <li>功能：oracle&gt;mysql&gt; nosql</li>
  <li>写性能：noSql&gt;oracle$\approx$mysql</li>
  <li>简单查询: oracle&gt;mysql&gt;nosql</li>
  <li>复杂查询(含join): oracle&gt;mysql&gt;nosql</li>
  <li>架构扩展: noSql&gt;mysql&gt;oracle</li>
  <li>可维护性: oracle&gt;mysql&gt;nosql</li>
  <li>成本: oracle&gt;mysql$\approx$nosql</li>
</ol>

<p>对于现在这个场景，爬虫在前端爬数据，管道层在那边处理数据后写数据，而这些数据具有时效性，也就是说只会去读一部分数据，相对来说，这就对写的要求较高。此外，这个场景就一个表，不涉及多表关联、约束等，复杂查询可以说没有，需要功能较少。另外网络数据不能保证一致性和可靠性，只要高可用性(HA)即可，Nosql可以设置副本机制达到高可用性，mysql虽然也可以做到成本稍高，将来可扩展角度也不适合。因此这个场景最适合的是Nosql。</p>

<h3 id="hbase-还是mongodb">Hbase 还是Mongodb</h3>
<p><a href="http://www.jdon.com/46128">Cassandra HBase和MongoDb性能比较</a>此文详细比较了三种主流Nosql数据库，最终项目选择Mongodb，就在于MongoDB适合做读写分离场景中的读取场景，并且其用js开发的，对json插入支持特别好。什么时候mongodb是较坏的选择呢，参考<a href="https://blog.scrapinghub.com/2013/05/13/mongo-bad-for-scraped-data/">WHY MONGODB IS A BAD CHOICE FOR STORING OUR SCRAPED DATA</a></p>

<p>python的mongodbSDK包叫pymongo，十分钟看个<a href="http://api.mongodb.com/python/current/tutorial.html#indexing">教程</a>就会了，这个业务场景为了加快查询，需要对价格、行政区、发布时间创建索引，其中价格、行政区由于是数组形式所以是多键索引，索引属性是稀疏的，即不允许空值。此外对这条微博的mblog_id加个唯一索引。索引在初始运行时创建，之后除非手动删除数据库后运行，否则不会再创建。</p>

<p>为保证每次插入的数据都是最新的，插入前应比较数据的发布时间与数据库中的最新时间，如果是早的说明已经爬过的，不需要插入。</p>

<p>关于mongodb的使用文档，点<a href="https://docs.mongodb.com/manual/indexes/">这里</a>。</p>

<h3 id="运行项目">运行项目</h3>

<p>将项目git到本地后，请先确保以下环境已经安装：</p>

<ul>
  <li><a href="https://scrapy.org">scrapy</a></li>
  <li><a href="https://github.com/pytries/datrie">datrie</a></li>
  <li><a href="https://github.com/mongodb/mongo-python-driver">pymongo</a></li>
  <li><a href="https://www.mongodb.org">mongoDB</a></li>
</ul>

<p>执行下面命令：</p>

<blockquote>
  <p>mongod<br />
cd weiboSA<br />
scrapy crawl mblogSpider</p>
</blockquote>

<p>可选参数：</p>
<blockquote>
  <p>scrapy crawl mblogSpider  -a num=     -a new_url=</p>
</blockquote>

<ul>
  <li>num 代表爬取页面数，默认为100页，目前只支持100页。</li>
  <li>new_url 默认为搜索移动端‘上海租房’返回的json文件url，如果要添加其他上海租房信息，比如浦东租房，请自行在Chrome中找到请求的json地址，例如：</li>
  <li>http://m.weibo.cn/page/pageJson?   <br />
containerid=&amp;containerid=100103type%3D1%26q%3D浦东租房<br />
&amp;type=all <br />
&amp;queryVal=浦东租房 <br />
&amp;luicode=10000011 <br />
&amp;lfid=100103type%3D%26q%3D上海无中介租房 <br />
&amp;title=浦东租房 <br />
&amp;v_p=11 <br />
&amp;ext=<br />
&amp;fid=100103type%3D1%26q%3D浦东租房<br />
&amp;uicode=10000011<br />
&amp;next_cursor=<br />
&amp;page=<br />
如果要数据库收录‘浦东租房’历史记录信息，请将<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/pipelines.py">pipelines.py</a>第87、88行注释掉。一般如果有‘上海租房’了就不要去搜索‘浦东租房’，因为基本上有‘浦东租房’的微博都会有@‘上海租房’，所以下面会出现插入重复记录错误。</li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17</pre></td><td class="code"><pre>➜  weiboZ git:(master) ✗ scrapy crawl mblogSpider -a num=10 -a new_url="http://m.weibo.cn/page/pageJson\?containerid\=\&amp;containerid\=100103type%3D1%26q%3D%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&amp;type\=all\&amp;queryVal\=%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&amp;luicode\=10000011\&amp;lfid\=100103type%3D%26q%3D%E4%B8%8A%E6%B5%B7%E6%97%A0%E4%B8%AD%E4%BB%8B%E7%A7%9F%E6%88%BF\&amp;title\=%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&amp;v_p\=11\&amp;ext\=\&amp;fid\=100103type%3D1%26q%3D%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&amp;uicode\=10000011\&amp;next_cursor\=\&amp;page\="
2016-10-29 14:41:11 [root] WARNING: 生成MongoPipeline对象
2016-10-29 14:41:11 [root] WARNING: 开始spider
2016-10-29 14:41:11 [root] WARNING: 允许插入数据的时间大于2016-10-29 14:15:05.875000
2016-10-29 14:41:13 [root] WARNING: do page1.
2016-10-29 14:41:13 [root] WARNING: do other pages.
2016-10-29 14:41:13 [root] ERROR: 编号为:E91f233Ds的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef4ri5bC6的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3UNqMmV的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3stkA8a的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3pzmJ6i的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef1OBtvQr的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef03Lj54z的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeYLU2GQd的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeYlBv7bn的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeXkop2vu的数据插入异常
2016-10-29 14:41:15 [root] WARNING: 结束spider
</pre></td></tr></tbody></table>
</div>
</div>

<p>更改日志显示级别请在<a href="https://github.com/luzhijun/weiboSA/blob/master/weiboZ/settings.py">setting.py</a>中修改LOG_LEVEL，介意采用项目默认的WARNNING，否则信息会很多。</p>

<p><strong>查询示例</strong></p>

<p>查询当前时区的2016-10-20至今有在9号线附近租房房租不高于2000的信息。</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><table style="border-spacing: 0"><tbody><tr><td class="gutter gl" style="text-align: right"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39</pre></td><td class="code"><pre><span class="nx">db</span><span class="p">.</span><span class="nx">house</span><span class="p">.</span><span class="nx">find</span><span class="p">(</span>
<span class="p">{</span>
	<span class="na">created_at</span><span class="p">:{</span><span class="na">$gt</span><span class="p">:</span><span class="k">new</span> <span class="nb">Date</span><span class="p">(</span><span class="s1">'2016-10-20T00:00:00'</span><span class="p">)},</span>
	<span class="na">$or</span><span class="p">:</span>
		<span class="p">[</span>
			<span class="p">{</span><span class="na">price</span><span class="p">:{</span><span class="na">$lte</span><span class="p">:</span><span class="mi">2000</span><span class="p">}},</span>
			<span class="p">{</span><span class="na">price</span><span class="p">:[]}</span>
		<span class="p">],</span>
	<span class="na">admin</span><span class="p">:</span><span class="s1">'9号线'</span><span class="p">,</span>
	<span class="na">tag</span><span class="p">:</span><span class="kc">true</span>
<span class="p">},</span>
<span class="p">{</span>	
	<span class="na">_id</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
	<span class="na">text</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
	<span class="na">created_at</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
	<span class="na">scheme</span><span class="p">:</span><span class="mi">1</span>
<span class="p">}</span>
<span class="p">).</span><span class="nx">hint</span><span class="p">(</span><span class="s1">'created_at_-1'</span><span class="p">).</span><span class="nx">pretty</span><span class="p">()</span>

<span class="p">{</span>
	<span class="s2">"text"</span> <span class="p">:</span> <span class="s2">"房子在大上海国际花园，漕宝路1555弄，距9号线合川路地铁站步行5分钟，距徐家汇站只有4站，现在转租大床，有独立卫生间，公共厨房，房租2400，平摊下来1200，有一女室友，室友宜家上班，限女生，没有物业费，包网络，水电自理@上海租房无中介 @上海租房无中介 @上海租房 @上海租房无中介联盟"</span><span class="p">,</span>
	<span class="s2">"scheme"</span> <span class="p">:</span> <span class="s2">"http://m.weibo.cn/1641537045/EetVm3WBV?"</span><span class="p">,</span>
	<span class="s2">"created_at"</span> <span class="p">:</span> <span class="nx">ISODate</span><span class="p">(</span><span class="s2">"2016-10-25T09:18:00Z"</span><span class="p">)</span>
<span class="p">}</span>
<span class="p">{</span>
	<span class="s2">"text"</span> <span class="p">:</span> <span class="s2">"#上海租房##上海出租#9号线松江泗泾地铁站金地自在城，12层，步行、公交或小区班车直达地铁站。精装，品牌家具家电，主卧1800RMB/月；公寓门禁出入，房东直租，电话：13816835869，或QQ：36804408。@上海租房 @互助租房 @房天下上海租房 @上海租房无中介   @应届毕业生上海租房"</span><span class="p">,</span>
	<span class="s2">"scheme"</span> <span class="p">:</span> <span class="s2">"http://m.weibo.cn/1641537045/Een8cAoy8?"</span><span class="p">,</span>
	<span class="s2">"created_at"</span> <span class="p">:</span> <span class="nx">ISODate</span><span class="p">(</span><span class="s2">"2016-10-24T16:00:00Z"</span><span class="p">)</span>
<span class="p">}</span>
<span class="p">{</span>
	<span class="s2">"text"</span> <span class="err">:</span> <span class="s2">"#上海租房# 个人离开上海：转租地铁9号线朝南主卧带大阳台，离地铁站两分钟！设备齐全，交通方便，随时入住。具体信息看图片～@上海租房 @上海租房无中介联盟 @魔都租房 帮转谢谢！"</span><span class="p">,</span>
	<span class="s2">"scheme"</span> <span class="err">:</span> <span class="s2">"http://m.weibo.cn/1641537045/EdRpfuKuH?"</span><span class="p">,</span>
	<span class="s2">"created_at"</span> <span class="err">:</span> <span class="nx">ISODate</span><span class="p">(</span><span class="s2">"2016-10-21T07:14:00Z"</span><span class="p">)</span>
<span class="p">}</span>
<span class="p">{</span>
	<span class="s2">"text"</span> <span class="err">:</span> <span class="s2">"9号线桂林路 离地铁站8分钟 招女生室友哦 @上海租房 @上海租房无中介联盟 上海·南京西路"</span><span class="p">,</span>
	<span class="s2">"scheme"</span> <span class="err">:</span> <span class="s2">"http://m.weibo.cn/1641537045/EdJ2U8Kv3?"</span><span class="p">,</span>
	<span class="s2">"created_at"</span> <span class="err">:</span> <span class="nx">ISODate</span><span class="p">(</span><span class="s2">"2016-10-20T09:57:00Z"</span><span class="p">)</span>
<span class="p">}</span>
</pre></td></tr></tbody></table>
</div>
</div>

<h2 id="note">Note</h2>

<ol>
  <li>python 的第三方requests库使用起来比自带的urllib更容易，是对urlib的进一步封装，读者可以自己尝试，这里不再举例。</li>
  <li>在spider文件夹目录下可自建爬虫，爬取像豆瓣租房小组类似信息加入数据库。</li>
  <li>数据分析部分比如如何识别微博机器人，如何构建信息评价指标等，每个人实现方案不一样，挖掘信息的程度不同而已，本文不予给出。</li>
  <li>可设置定时任务，比如一般上海租房每天更新两页，就定时运行命令并且num=2。</li>
  <li>技术分享，全篇五千多字欢迎转载，但请注明出处，否则，否则我哭给你看😢。。。</li>
</ol>

<h2 id="ref">Ref</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Web scraping with python, http://shop.oreilly.com/product/0636920034391.do&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Twisted网络编程必备, http://down.51cto.com/data/616351&nbsp;<a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>THEPPITAKK.Animplementationofdouble-araytrie[z].http:/Ainux.thai.net/–thep/datrie/datrie.html，2006.&nbsp;<a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>双数组Tire树简介. http://www.cnblogs.com/ooon/p/4883159.html&nbsp;<a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>王思力，张华平，王斌.双数组Trie树算法优化及其应用研究[J].中文信息学报，2006，20(5):24—30&nbsp;<a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


                <hr>

                <!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2144688"></script>
<!-- UY END -->

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2016/10/19/google+%E6%96%87%E6%9C%AC%E6%8F%90%E5%8F%96%E5%92%8C%E5%88%86%E6%9E%90/" data-toggle="tooltip" data-placement="top" title="Google Plus 文本提取与分析">&larr; Previous Post</a>
                    </li>
                    
                    
                </ul>

            </div>

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#前端开发" title="前端开发" rel="2">
                                    前端开发
                                </a>
                            
        				
                            
                				<a href="/tags/#JavaScript" title="JavaScript" rel="2">
                                    JavaScript
                                </a>
                            
        				
                            
        				
                            
                				<a href="/tags/#社会计算" title="社会计算" rel="4">
                                    社会计算
                                </a>
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
        				
                            
                				<a href="/tags/#算法" title="算法" rel="3">
                                    算法
                                </a>
                            
        				
                            
                				<a href="/tags/#大数据" title="大数据" rel="3">
                                    大数据
                                </a>
                            
        				
                            
        				
                            
                				<a href="/tags/#社交网站数据分析" title="社交网站数据分析" rel="3">
                                    社交网站数据分析
                                </a>
                            
        				
                            
        				
        			</div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://blog.csdn.net/trucyluce">Trucy CSDN Blog</a></li>
                    
                        <li><a href="http://echoma.github.io/text_sequence_diagram/">在线时序图</a></li>
                    
                        <li><a href="http://naotu.baidu.com/">在线思维导图</a></li>
                    
                        <li><a href="http://www.cnblogs.com/baiboy/">NLP伏草惟存</a></li>
                    
                        <li><a href="http://blog.5long.me/">茶码话桑麻</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>

<div id="lv-container" data-id="city" data-uid="MTAyMC8zMDkwOC83NDU3">
    <script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
    </script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>







<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("http://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    
                    

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/trucyluce">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="http://weibo.com/TrucyLuce">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    


                    
                    <li>
                        <a target="_blank" href="https://www.facebook.com/TrucyLuce">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li>
                        <a target="_blank" href="https://github.com/luzhijun">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; T.L Blog 2017
                    <br>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("http://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-83947622-1';
    var _gaDomain = 'https://luzhijun.github.io';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'c07b62da6a94126e41877e6b30e15413';

    // Originial
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?" + _baId;
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
</script>

<!-- 代码行号-->
<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>
<script >
!function(){"undefined"!=typeof self&&self.Prism&&self.document&&Prism.hooks.add("complete",function(e){if(e.code){var t=e.element.parentNode,s=/\s*\bhighlight\b\s*/;if(t&&/pre/i.test(t.nodeName)&&(s.test(t.className)||s.test(e.element.className))&&!e.element.querySelector(".line-numbers-rows")){s.test(e.element.className)&&(e.element.className=e.element.className.replace(s,"")),s.test(t.className)||(t.className+="highlight");var n,a=e.code.match(/\n(?!$)/g),l=a?a.length+1:1,m=new Array(l+1);m=m.join("<span></span>"),n=document.createElement("span"),n.className="line-numbers-rows",n.innerHTML=m,t.hasAttribute("data-start")&&(t.style.counterReset="linenumber "+(parseInt(t.getAttribute("data-start"),10)-1)),e.element.appendChild(n)}}})}();
</script>


<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
